{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "DEFAULT_WIDTH_MULTIPLIER = 2.0\n",
    "DEFAULT_MIN_DEXTRA_LAYERS = 4\n",
    "DEFAULT_MAX_DEXTRA_LAYERS = 8\n",
    "DEFAULT_BASE_GROUPS = 16\n",
    "MIN_ELEMENTS_PER_GROUP = 32\n",
    "DEFAULT_FFN_RED_FACTOR = 4\n",
    "DEFAULT_DROPOUT = 0.1\n",
    "DEFAULT_STD_DROPOUT = 0.1\n",
    "ADAPTIVE_SCALE_FACTOR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# print(args)\\nfrom fake_bert.models.dexTraUnit import DExTraUnit\\n\\nDExTraUnit(\\n    in_features=args.delight_emb_map_dim,\\n    in_proj_features=args.delight_emb_out_dim // 2,\\n    out_features=args.delight_emb_out_dim,\\n    width_multiplier=args.delight_emb_width_mult,\\n    dextra_depth=args.delight_emb_depth,\\n    dextra_dropout=args.delight_dropout,\\n    max_glt_groups=args.delight_emb_max_groups,\\n    act_type=args.act_type,\\n    norm_type=args.norm_type,\\n    use_bias=True,\\n    is_iclr_version=args.define_iclr,\\n    glt_shuffle=args.glt_shuffle,\\n)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Import Lib\n",
    "\n",
    "\n",
    "\n",
    "def base_architecture(args):\n",
    "    # DeLighT Embedding layer\n",
    "    args.adaptive_input = getattr(args, \"adaptive_input\", False)\n",
    "    args.delight_emb_map_dim = getattr(args, \"delight_emb_map_dim\", 128)\n",
    "    args.delight_emb_out_dim = getattr(args, \"delight_emb_out_dim\", 128)\n",
    "    # compute the max groups in GLT\n",
    "    assert args.delight_emb_out_dim % MIN_ELEMENTS_PER_GROUP == 0, 'remainder({}, {}) should be equal to 0'.format(\n",
    "        args.delight_emb_out_dim, MIN_ELEMENTS_PER_GROUP)\n",
    "    max_groups = 2 ** math.ceil(math.log(args.delight_emb_out_dim // MIN_ELEMENTS_PER_GROUP, 2))\n",
    "\n",
    "    args.delight_emb_max_groups = getattr(args, \"delight_emb_max_groups\", max_groups)\n",
    "    args.delight_emb_dropout = getattr(args, \"delight_emb_dropout\", DEFAULT_DROPOUT)\n",
    "    args.delight_emb_depth = getattr(args, \"delight_emb_depth\", DEFAULT_MIN_DEXTRA_LAYERS)\n",
    "    args.delight_emb_width_mult = getattr(args, \"delight_emb_width_mult\", DEFAULT_WIDTH_MULTIPLIER)\n",
    "\n",
    "    # Encoder arguments in DeLighT\n",
    "    args.delight_enc_scaling = getattr(args, \"delight_enc_scaling\", 'block')\n",
    "    args.delight_enc_layers = getattr(args, \"delight_enc_layers\", DEFAULT_MAX_DEXTRA_LAYERS)\n",
    "    args.delight_enc_min_depth = getattr(args, \"delight_enc_min_depth\", DEFAULT_MIN_DEXTRA_LAYERS)\n",
    "    args.delight_enc_max_depth = getattr(args, \"delight_enc_max_depth\", DEFAULT_MAX_DEXTRA_LAYERS)\n",
    "    args.delight_enc_width_mult = getattr(args, \"delight_enc_width_mult\", DEFAULT_WIDTH_MULTIPLIER)\n",
    "    args.delight_enc_ffn_red = getattr(args, \"delight_enc_ffn_red\", DEFAULT_FFN_RED_FACTOR)\n",
    "    args.delight_enc_max_groups = getattr(args, \"delight_enc_max_groups\", max_groups)\n",
    "\n",
    "    # Decoder arguments in DeLighT\n",
    "    args.delight_dec_scaling = getattr(args, \"delight_dec_scaling\", 'block')\n",
    "    args.delight_dec_layers = getattr(args, \"delight_dec_layers\", DEFAULT_MAX_DEXTRA_LAYERS)\n",
    "    args.delight_dec_min_depth = getattr(args, \"delight_dec_min_depth\", DEFAULT_MIN_DEXTRA_LAYERS)\n",
    "    args.delight_dec_max_depth = getattr(args, \"delight_dec_max_depth\", DEFAULT_MAX_DEXTRA_LAYERS)\n",
    "    args.delight_dec_width_mult = getattr(args, \"delight_dec_width_mult\", DEFAULT_WIDTH_MULTIPLIER)\n",
    "    args.delight_dec_ffn_red = getattr(args, \"delight_dec_ffn_red\", DEFAULT_FFN_RED_FACTOR)\n",
    "    args.delight_dec_max_groups = getattr(args, \"delight_dec_max_groups\", max_groups)\n",
    "\n",
    "    ## Others\n",
    "    args.no_glt_shuffle = getattr(args, \"no_glt_shuffle\", False)\n",
    "    args.glt_shuffle = not args.no_glt_shuffle\n",
    "    args.define_iclr = getattr(args, \"define_iclr\", False)\n",
    "    args.delight_dropout = getattr(args, \"delight_dropout\", DEFAULT_DROPOUT)\n",
    "\n",
    "    # normalization and activation layers\n",
    "    args.norm_type = getattr(args, \"norm_type\", 'ln')\n",
    "    args.act_type = getattr(args, \"act_type\", 'swish')\n",
    "\n",
    "    # ADAPTIVE INPUT AND OUTPUT PARAMS\n",
    "    args.adaptive_softmax_cutoff = getattr(args, \"adaptive_softmax_cutoff\", None)\n",
    "    args.adaptive_softmax_dropout = getattr(args, \"adaptive_softmax_dropout\", 0)\n",
    "    args.adaptive_softmax_factor = getattr(args, 'adaptive_softmax_factor', 4)\n",
    "    args.tie_adaptive_weights = getattr(args, 'tie_adaptive_weights', False)\n",
    "    args.tie_adaptive_proj = getattr(args, 'tie_adaptive_proj', False)\n",
    "\n",
    "    # Print  stats\n",
    "    args.print_stats = getattr(args, \"print_stats\", False)\n",
    "    args.src_len_ps = getattr(args, \"src_len_ps\", 20)\n",
    "    args.tgt_len_ps = getattr(args, \"tgt_len_ps\", 20)\n",
    "\n",
    "    # DROPOUTS\n",
    "    args.attention_dropout = getattr(args, \"attention_dropout\", DEFAULT_DROPOUT)\n",
    "    args.activation_dropout = getattr(args, \"activation_dropout\", 0.0)\n",
    "    args.dropout = getattr(args, \"dropout\", DEFAULT_DROPOUT)\n",
    "    args.delight_dropout = getattr(args, \"delight_dropout\", 0.0)\n",
    "    args.pe_dropout = getattr(args, \"pe_dropout\", DEFAULT_DROPOUT)\n",
    "    args.ffn_dropout = getattr(args, \"ffn_dropout\", DEFAULT_DROPOUT)\n",
    "\n",
    "    # Other parameters\n",
    "    args.encoder_normalize_before = getattr(args, \"encoder_normalize_before\", False)\n",
    "    args.decoder_normalize_before = getattr(args, \"decoder_normalize_before\", False)\n",
    "\n",
    "    args.share_decoder_input_output_embed = getattr(\n",
    "        args, \"share_decoder_input_output_embed\", False\n",
    "    )\n",
    "    args.share_all_embeddings = getattr(args, \"share_all_embeddings\", False)\n",
    "    args.no_token_positional_embeddings = getattr(\n",
    "        args, \"no_token_positional_embeddings\", False\n",
    "    )\n",
    "    args.encoder_learned_pos = getattr(args, 'encoder_learned_pos', False)\n",
    "    args.decoder_learned_pos = getattr(args, 'decoder_learned_pos', False)\n",
    "\n",
    "    args.no_scale_embedding = getattr(args, \"no_scale_embedding\", False)\n",
    "\n",
    "args = argparse.Namespace()\n",
    "base_architecture(args)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# print(args)\n",
    "from fake_bert.models.dexTraUnit import DExTraUnit\n",
    "\n",
    "DExTraUnit(\n",
    "    in_features=args.delight_emb_map_dim,\n",
    "    in_proj_features=args.delight_emb_out_dim // 2,\n",
    "    out_features=args.delight_emb_out_dim,\n",
    "    width_multiplier=args.delight_emb_width_mult,\n",
    "    dextra_depth=args.delight_emb_depth,\n",
    "    dextra_dropout=args.delight_dropout,\n",
    "    max_glt_groups=args.delight_emb_max_groups,\n",
    "    act_type=args.act_type,\n",
    "    norm_type=args.norm_type,\n",
    "    use_bias=True,\n",
    "    is_iclr_version=args.define_iclr,\n",
    "    glt_shuffle=args.glt_shuffle,\n",
    ")\n",
    "\"\"\"\n",
    "# self.dextra_layer = DExTraUnit(\n",
    "#             in_features=self.input_features,\n",
    "#             in_proj_features=self.embedding_dim // 2,\n",
    "#             out_features=self.embedding_dim,\n",
    "#             width_multiplier=args.delight_emb_width_mult,\n",
    "#             dextra_depth=args.delight_emb_depth,\n",
    "#             dextra_dropout=args.delight_dropout,\n",
    "#             max_glt_groups=args.delight_emb_max_groups,\n",
    "#             act_type=args.act_type,\n",
    "#             norm_type=args.norm_type,\n",
    "#             use_bias=use_bias,\n",
    "#             is_iclr_version=args.define_iclr,\n",
    "#             glt_shuffle=args.glt_shuffle,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Embbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fake_bert.models.dextra_emb import DExTraEmb\n",
    "from fake_bert.models.nn_functions import get_embedding_layer\n",
    "from fake_bert.datasets.dataset import RandomGenerator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "padding_idx = 3\n",
    "num_embeddings = 150\n",
    "# delight_emb_map_dim = 128\n",
    "\n",
    "map_layer = get_embedding_layer(num_embeddings=num_embeddings,\n",
    "                embedding_dim=args.delight_emb_map_dim,\n",
    "                padding_idx=padding_idx)\n",
    "emb = DExTraEmb(args, map_layer=map_layer)\n",
    "\n",
    "dataset = RandomGenerator()\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=False)\n",
    "data_sample = next(iter(dataloader))\n",
    "x = data_sample[0]\n",
    "\n",
    "out = emb(x)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from fake_bert.models.nn_functions import get_embedding_layer\n",
    "from fake_bert.datasets.dataset import RandomGenerator\n",
    "from torch.utils.data import DataLoader\n",
    "from fake_bert.models.dextra_emb import DExTraEmb\n",
    "from torch import Tensor\n",
    "from fake_bert.models.dextra_unit import DExTraUnit\n",
    "model = nn.TransformerEncoderLayer(128, 1, 64)\n",
    "\n",
    "padding_idx = 3\n",
    "num_embeddings = 150\n",
    "# delight_emb_map_dim = 128\n",
    "\n",
    "class DeLightEncoderLayer(nn.TransformerEncoderLayer):\n",
    "    \"\"\"Some Information about DeLightEncoderLayer\"\"\"\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, layer_norm_eps: float = 0.00001, batch_first: bool = False, norm_first: bool = False, device=None, dtype=None) -> None:\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, device, dtype)\n",
    "\n",
    "        self.dextra_layer = DExTraUnit(\n",
    "            in_features=args.delight_emb_map_dim,\n",
    "            in_proj_features=args.delight_emb_out_dim // 2,\n",
    "            out_features=args.delight_emb_out_dim,\n",
    "            width_multiplier=args.delight_emb_width_mult,\n",
    "            dextra_depth=args.delight_emb_depth,\n",
    "            dextra_dropout=args.delight_dropout,\n",
    "            max_glt_groups=args.delight_emb_max_groups,\n",
    "            act_type=args.act_type,\n",
    "            norm_type=args.norm_type,\n",
    "            use_bias=True,\n",
    "            is_iclr_version=args.define_iclr,\n",
    "            glt_shuffle=args.glt_shuffle,\n",
    "        )\n",
    "\n",
    "    def forward_dextra(self, x):\n",
    "        x = self.dextra_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        # print(src.shape)\n",
    "        # print(src.dtype)\n",
    "        src = self.forward_dextra(src)\n",
    "        # print(src.shape)\n",
    "        # print(src.dtype)\n",
    "        return super().forward(src, src_mask, src_key_padding_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.TransformerEncoderLayer):\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = ..., layer_norm_eps: float = 0.00001, batch_first: bool = False, norm_first: bool = False, device=None, dtype=None) -> None:\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.TransformerEncoderLayer(d_model=128, nhead=1, dim_feedforward=64)\n",
    "model(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeLightEncoderLayer(d_model=128, nhead=1, dim_feedforward=64)\n",
    "model(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0052e534147cfcca1f872934f13253e744d5a774d345f54e514f151c2a55968a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
